{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial load\n",
    "\n",
    "This notebook initially loads the datalets:\n",
    "- nBew_Jakobcsv\n",
    "- patientenId_Jakob\n",
    "- sturzd_Jakob\n",
    "\n",
    "Furthermore, the otebook processes (some of the) files and saves them as .pks files. This makes them faster to load. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import csv\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "\n",
    "*csv_processer* is needed to process sturzd_Jakob, since the file uses ';' as separater but in some of the text boxes of the file, the same symbol is used in a text. Therefore this funciton takes the csv, and whenever something is in quotaiton, the processer changes the ';' to the inQuotesSep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_processer(inputFile, outputFile, inQuotesSep, sep=';'):\n",
    "    # Open input CSV file for reading\n",
    "    with open(inputFile, 'r') as f:\n",
    "        # Create CSV reader object using specified delimiter\n",
    "        reader = csv.reader(f, delimiter=sep)\n",
    "        \n",
    "        # Open output CSV file for writing\n",
    "        with open(outputFile, 'w', newline='') as out:\n",
    "            # Create CSV writer object using specified delimiter\n",
    "            writer = csv.writer(out, delimiter=sep)\n",
    "            \n",
    "            # Iterate over each row in the input CSV file\n",
    "            for row in reader:\n",
    "                # Create a new row to store modified field values\n",
    "                newRow = []\n",
    "                \n",
    "                # Iterate over each field in the current row\n",
    "                for field in row:\n",
    "                    #field = field.replace('\"', '')\n",
    "                    # If the separator character is found in the field\n",
    "                    if sep in field:\n",
    "                        # Replace the separator with specified character\n",
    "                        field_new = field.replace(sep, inQuotesSep)\n",
    "                        # Append modified field to the new row\n",
    "                        newRow.append(field_new)\n",
    "                    else:\n",
    "                        # If separator character not found, append field to new row as is\n",
    "                        newRow.append(field)\n",
    "                \n",
    "                # Write the new row to the output CSV file\n",
    "                writer.writerow(newRow)\n",
    "\n",
    "# Example usage:\n",
    "# processer('input.csv', 'output.csv', '|')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed sturzd in  0.08  sec\n"
     ]
    }
   ],
   "source": [
    "#preprocess and import sturzd\n",
    "s= time.time()\n",
    "csv_processer(\"data/src/sturzd_Jakob.csv\",\"data/src/sturzd_clean.csv\",\"|\")\n",
    "end1 =time.time()\n",
    "print(\"processed sturzd in \", str(round(end1-s,2)),\" sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imported PatID in  1.75  sec\n",
      "imported nBew in  10.48  sec\n"
     ]
    }
   ],
   "source": [
    "#load patID and nBew files\n",
    "s =time.time()\n",
    "patID = pd.read_csv(\"data/src/patientenId_Jakob.csv\", delimiter=';')\n",
    "end1 =time.time()\n",
    "print(\"imported PatID in \",str(round(end1-s,2)), \" sec\")\n",
    "nBew = pd.read_csv(\"data/src/nBew_Jakob.csv\", delimiter=';')\n",
    "end2 =time.time()\n",
    "print(\"imported nBew in \",str(round(end2-end1,2)), \" sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported sturzd in  0.04  sec\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "fallD =pd.read_csv(\"data/src/sturzd_clean.csv\", delimiter=';')\n",
    "end =time.time()\n",
    "print('Imported sturzd in ',str(round(end-s,2)),' sec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# checking a couple of numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of cases:  3332640\n",
      "number of patients:  932103\n",
      "number of cases per patient:  3.58\n"
     ]
    }
   ],
   "source": [
    "n_cases = patID.shape[0]\n",
    "n_pat = patID['c_patnr_pseudonym'].nunique()\n",
    "print('number of cases: ',n_cases)\n",
    "print('number of patients: ', n_pat )\n",
    "print('number of cases per patient: ', round(n_cases/n_pat,2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of movements:  10689561\n",
      "number of movements per case:  3.21\n"
     ]
    }
   ],
   "source": [
    "n_movements = nBew.shape[0]\n",
    "print('number of movements: ', n_movements)\n",
    "print('number of movements per case: ', round(n_movements/n_cases,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of falls:  14556\n"
     ]
    }
   ],
   "source": [
    "n_fall = fallD.shape[0]\n",
    "print('number of falls: ', n_fall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nBew['c_orgfa'].nunique()\n",
    "\n",
    "# Extract unique values\n",
    "unique_values = nBew['c_orgfa'].unique()\n",
    "\n",
    "# Convert the unique values to a DataFrame\n",
    "unique_values_df = pd.DataFrame(unique_values, columns=['c_orgfa'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# invstigation time Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date strings to datetime objects\n",
    "date_start = datetime.strptime('2016-05-01', '%Y-%m-%d')\n",
    "date_end = datetime.strptime('2022-04-30', '%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nBew "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filter all movements so that only the falls between 2016 and 2022 are in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows removed: 1355228\n",
      "Number of rows remaining: 9334333\n",
      "Percentage of rows remaining: 87.32%\n"
     ]
    }
   ],
   "source": [
    "#convert to datetime from UNIX timestamp\n",
    "nBew['from_time_converted'] = nBew['from_time'].apply(datetime.fromtimestamp)\n",
    "nBew['till_time_converted'] = nBew['till_time'].apply(datetime.fromtimestamp)\n",
    "\n",
    "before = nBew.shape[0]\n",
    "\n",
    "\n",
    "# Filter rows based on date range\n",
    "nBew = nBew[(nBew['from_time_converted'] >= date_start) & (nBew['from_time_converted'] <= date_end)]\n",
    "nBew = nBew[(nBew['till_time_converted'] >= date_start) & (nBew['till_time_converted'] <= date_end)]\n",
    "\n",
    "\n",
    "after = nBew.shape[0]\n",
    "\n",
    "print(f\"Number of rows removed: {before - after}\")\n",
    "print(f\"Number of rows remaining: {after}\")\n",
    "print(f\"Percentage of rows remaining: {after / before * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sturzD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filter all falls so that only the falls between 2016 and 2022 are in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows removed: 296\n",
      "Number of rows remaining: 14260\n",
      "Percentage of rows remaining: 97.97%\n"
     ]
    }
   ],
   "source": [
    "#convert to datetime from UNIX timestamp\n",
    "fallD['Sturz_datetime_converted'] = fallD['Sturz_datetime'].apply(datetime.fromtimestamp)\n",
    "before = fallD.shape[0]\n",
    "\n",
    "# Filter rows based on date range\n",
    "fallD = fallD[(fallD['Sturz_datetime_converted'] >= date_start) & (fallD['Sturz_datetime_converted'] <= date_end)]\n",
    "\n",
    "after = fallD.shape[0]\n",
    "\n",
    "fallD['Sturz_datetime_converted'] = pd.to_datetime(fallD['Sturz_datetime_converted'])\n",
    "\n",
    "print(f\"Number of rows removed: {before - after}\")\n",
    "print(f\"Number of rows remaining: {after}\")\n",
    "print(f\"Percentage of rows remaining: {after / before * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fallD['Hour'] = fallD['Sturz_datetime'].apply(lambda ts: datetime.fromtimestamp(ts).hour)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valididy checks\n",
    "\n",
    "checks for validity and adjusts if needed. The follwoing should apply\n",
    "\n",
    "**patID**: \n",
    "- c_pseudonym must be unique\n",
    "\n",
    "**nBew**:\n",
    "- for each c_pseudonym, the line times must not with from_time and till_time\n",
    "- c_lfdnr must be increasing and without misses\n",
    "\n",
    "**sturzD**: \n",
    "- Sturz_datetime not exactly twice\n",
    "- Sturz_datetime within time of stay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### patID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patID is unique:  False\n"
     ]
    }
   ],
   "source": [
    "patID_unique_ID= patID['c_pseudonym'].is_unique\n",
    "print('patID is unique: ', patID_unique_ID)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get not unique id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 duplicates\n",
      "-> there are  1  cases that are associated to more then one patient\n"
     ]
    }
   ],
   "source": [
    "duplicates_ID = patID[patID.duplicated(subset=['c_pseudonym'])]\n",
    "print('There are',duplicates_ID.shape[0], 'duplicates')\n",
    "if duplicates_ID.shape[0]>0:\n",
    "    print('-> there are ', duplicates_ID.shape[0],' cases that are associated to more then one patient')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all instances of that one duplicate\n",
    "duplicated_ID = duplicates_ID['c_pseudonym'].tolist()[0]\n",
    "duplicates = patID[patID['c_pseudonym']==duplicated_ID]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350670: 1\n",
      "769976: 1\n"
     ]
    }
   ],
   "source": [
    "# check if the patient number that is used in the duplicates is unique, if only 1 then unique\n",
    "for pseudonym in duplicates['c_patnr_pseudonym']:\n",
    "    count = patID[patID['c_patnr_pseudonym'] == pseudonym]['c_patnr_pseudonym'].count()\n",
    "    print(f\"{pseudonym}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "while there are two cases with the same case number, the corresponding patient (two distinct patiens) each only exist once. therefore, to fix the prblem we can just eliminate one of the duplicate.\n",
    "\n",
    "this only works, since patient number is nowere else used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#droping duplicates\n",
    "patID = patID.drop_duplicates(subset=['c_pseudonym'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patID is unique:  True\n"
     ]
    }
   ],
   "source": [
    "#check for duplicates\n",
    "patID_unique_ID= patID['c_pseudonym'].is_unique\n",
    "print('patID is unique: ', patID_unique_ID)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No duplicates anymore-> perfect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter to have only the cases that are in the movement\n",
    "\n",
    "Add Patient ID to \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper_nBew = nBew['c_pseudonym'].copy()\n",
    "helper_nBew = nBew['c_pseudonym'].copy().drop_duplicates()\n",
    "helper_nBew = helper_nBew.reset_index()\n",
    "helper_nBew['is_in_nBew'] = True\n",
    "helper_nBew = helper_nBew.drop(columns=['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows removed: 407775\n",
      "Number of rows remaining: 2924864\n",
      "Percentage of rows remaining: 87.76%\n"
     ]
    }
   ],
   "source": [
    "# Calculate the number of rows before filtering\n",
    "before = patID.shape[0]\n",
    "\n",
    "# Merge patID with helper_nBew\n",
    "patID = patID.merge(helper_nBew, on='c_pseudonym', how='left')\n",
    "\n",
    "# Filter patID where is_in_nBew is True\n",
    "patID = patID[patID['is_in_nBew'] == True].drop(columns=['is_in_nBew'])\n",
    "\n",
    "# Calculate the number of rows after filtering\n",
    "after = patID.shape[0]\n",
    "\n",
    "\n",
    "print(f\"Number of rows removed: {before - after}\")\n",
    "print(f\"Number of rows remaining: {after}\")\n",
    "print(f\"Percentage of rows remaining: {after / before * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "delete all patients from patientID that are not moving (blanks, something wrong, since admission is a movement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adjust specialitites to ignore different campus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "specialities = pd.read_pickle(\"data/specialities/specialities.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "specialities_mapping = pd.read_pickle('data/specialities/speciality_mapping.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = pd.merge(specialities, specialities_mapping, left_on='speciality', right_on='German', how='left').drop_duplicates(subset=['OEs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec.drop(columns=['speciality','German','English'], inplace=True)\n",
    "spec.rename(columns={'Adjusted Category':'speciality'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## temporary part start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of wards:  1510\n",
      "number of departments:  81\n"
     ]
    }
   ],
   "source": [
    "n_wards = spec['OEs'].nunique()\n",
    "n_departments = spec['speciality'].nunique()\n",
    "\n",
    "print('number of wards: ', n_wards)\n",
    "print('number of departments: ', n_departments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "213"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = pd.merge(unique_values_df, spec, left_on='c_orgfa', right_on='OEs', how='left')\n",
    "temp.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp['speciality'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## temporary part end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "nBew_temp = pd.merge(nBew, spec, left_on='c_orgfa', right_on='OEs', how='left')\n",
    "nBew_temp = nBew_temp.drop(columns=['OEs'])\n",
    "nBew_temp = nBew_temp.rename(columns={'c_orgfa': 'c_orgfa_org'})\n",
    "nBew_temp = nBew_temp.rename(columns={'speciality': 'c_orgfa'})\n",
    "nBew = nBew_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fallD_temp = pd.merge(fallD, spec, left_on='Fachliche Zuweisung', right_on='OEs', how='left')\n",
    "fallD_temp = fallD_temp.drop(columns=['OEs'])\n",
    "fallD_temp = fallD_temp.rename(columns={'Fachliche Zuweisung': 'Fachliche Zuweisung Alt'})\n",
    "fallD_temp = fallD_temp.rename(columns={'speciality': 'Fachliche Zuweisung'})\n",
    "fallD_temp['Fachliche Zuweisung'] = fallD_temp['Fachliche Zuweisung'].fillna('Unknown')\n",
    "fallD = fallD_temp\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File storing as PKL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store all files in .pkl\n",
    "patID.to_pickle(\"data/src_pkl/patID.pkl\")\n",
    "nBew.to_pickle(\"data/src_pkl/nBew.pkl\")\n",
    "fallD.to_pickle(\"data/src_pkl/sturzD.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "f4e9cda46bb2d9d7fe6ecdff0f8336a934348bf06cb492f2f42f60739b3403b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
